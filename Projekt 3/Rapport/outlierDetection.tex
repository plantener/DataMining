To be able to find anomalies and outliers we will in this section, use some different methods to be able find the most likely anomalies and outliers. Afterwards we will then compare the different methods we have done on our data, to get the most likely outliers from the different methods we have used.

Using leave-one-out, we were able to find the optimal kernel width to $0.25$.

\subsection{Density Estimation}

When we are checking our different data objects estimated density, we do not want our data to be in an low density area, since they are then most likely to be outliers.

When we look at a Gaussian density estimation of all our data objects we can see that we probably have below a 100 outliers, which we then afterwards only take a look at those 100. Here we see that after the 20th object the curve starts to grow, which means we can take a look at those, and when we look closer to those we see around the 17-19 that it have an significant growth which could mean that we have some outliers lying below these. This does not mean they are outliers, but it could be an indication of it.

\begin{figure}[H]
\centering
\includegraphics[width=5cm, keepaspectratio=true]{pictures/densityEstimationAll.png}
\includegraphics[width=5cm, keepaspectratio=true]{pictures/densityEstimation100.png}
\includegraphics[width=5cm, keepaspectratio=true]{pictures/densityEstimation20.png}
\vspace{-0.4cm}
\caption{\footnotesize Outlier score gaussian density estimation}
\label{gkd}
\end{figure}

With a KNN density estimation we want to have some of the same data object as with gassian density estimation to be in the low density end, so that we are able to confirm that we have some outliers. As with the Gaussian method, we again first look at figure of all the data objects, which shows a bit better then Gaussian where we have a sudden increase of density. In this case it already starts in the 1st data object and drastically increases until around the 50th data object. So after this we take a look at 100 lowest density scores, this actual shows that we have the most drastic increase around the 5'th lowest density data objects. We decrease the number of objects to look at, and this show that the 4 lowest densities most likely are outliers.

\begin{figure}[H]
\centering
\includegraphics[width=5cm, keepaspectratio=true]{pictures/knndensityEstimationAll.png}
\includegraphics[width=5cm, keepaspectratio=true]{pictures/knndensityEstimation100.png}
\includegraphics[width=5cm, keepaspectratio=true]{pictures/knndensityEstimation20.png}
\vspace{-0.4cm}
\caption{\footnotesize Outlier score knn density estimation}
\label{knn}
\end{figure}

As with Gaussian and KNN we start to look at all the data objects score in the knn average relative density, and see it increases quick in the beginning, so we take a look at the 20 lowest data objects. In this graph we the see that the from the data object with the lowest density to the next one we have the biggest jump in density, which most likely would make the data object with the lowest density an outlier.

\begin{figure}[H]
\centering
\includegraphics[width=7.5cm, keepaspectratio=true]{pictures/knnAvgdensityEstimationAll.png}
\includegraphics[width=7.5cm, keepaspectratio=true]{pictures/knnAvgdensityEstimation20.png}
\vspace{-0.4cm}
\caption{\footnotesize Outlier score knn average relative density estimation}
\label{avg}
\end{figure}

\subsection{K'th Nearest Neighbour Distance}

When we take a look at the distance method, we see that this time we want a low score in stead of a high score as with the distance scores. But it shows around the same numbers of bad scores as with the density as well as good scores, which can be seen in the table below. You can also see that it is some of the same data objects that keeps showing up in all the different runs, they can be seen in the table below.
\begin{table}[H]
\begin{longtable}{lccccc}
\hline
   & 1   & 2   & 3   & 4   & 5   \\ \hline
5  & 21  & 345 & 362 & 426 & 442 \\ 
10 & 345 & 21  & 362 & 426 & 8   \\ 
25 & 345 & 362 & 21  & 426 & 241 \\
50 & 345 & 362 & 241 & 21  & 359 \\ \hline
\end{longtable}
\caption{\footnotesize In this table we see k'th nearest neighbour distance, for different k values. Each row is a new run with a different k, and the columns is the 5 worst scores.}
\end{table}

\subsection{Comparison of outlier score}

When we compare the different outlier score and see which data objects the different methods found, we see that some of them are recurring. If we take a look at table \ref{compare} we see that the first data object in all 4 methods, all have data object 345, this could mean that it is an outlier since each method gave it a bad score. If we then take a look at some of the others, we see that some of them are recurring in some of the other methods, again this could mean that these are outliers to. If we take a look at the table below we can properly say that data object 345, 246, 362 and 241 is outliers because of they have a low score and is recurring in at least one of the other methods.
\begin{table}[H]
\begin{longtable}{lcccc}
\hline
  & Gaussian & KNN & AVG KNN & 25 Dist \\ \hline
1 & 345      & 345 & 345     & 345 \\ 
2 & 426      & 426 & 140     & 362 \\ 
3 & 362      & 362 & 141     & 21 \\ 
4 & 241      & 442 & 426     & 426 \\ 
5 & 334      & 241 & 39      & 241 \\ \hline
\end{longtable}
\label{compare}
\caption{\footnotesize Here we see the comparison of all the different outlier scoring methods.}
\end{table}